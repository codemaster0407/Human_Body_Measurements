{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d1fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries used \n",
    "#Xmax = 640, Ymax = 480\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe47c82",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     61\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 62\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Draw the pose annotation on the image.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "BG_COLOR = (192, 192, 192) # gray\n",
    "with mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=2,\n",
    "    enable_segmentation=True,\n",
    "    min_detection_confidence=0.5) as pose:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread('POSE 1.jpg')\n",
    "    image_height, image_width, _ = image.shape\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "      continue\n",
    "    print(\n",
    "        f'Nose coordinates: ('\n",
    "        f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x * image_width}, '\n",
    "        f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y * image_height})'\n",
    "    )\n",
    "\n",
    "    annotated_image = image.copy()\n",
    "    # Draw segmentation on the image.\n",
    "    # To improve segmentation around boundaries, consider applying a joint\n",
    "    # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "    bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "    bg_image[:] = BG_COLOR\n",
    "    annotated_image = np.where(condition, annotated_image, bg_image)\n",
    "    # Draw pose landmarks on the image.\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n",
    "    # Plot pose world landmarks.\n",
    "    mp_drawing.plot_landmarks(\n",
    "        results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "\n",
    "    # Draw the pose annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "keypoints = []\n",
    "for data_point in results.pose_landmarks.landmark:\n",
    "    keypoints.append({'X':data_point.x, 'Y': data_point.y, 'Z':data_point.z, 'visibility': data_point.visibility,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff6397d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have 33 key points in media pose dataset. \n",
    "\n",
    "\n",
    "len(keypoints) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e380e",
   "metadata": {},
   "source": [
    "Left arm pixels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7311ebcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.03513988645842 71.99305291968672\n"
     ]
    }
   ],
   "source": [
    "#For left arm (12, 14, 16)\n",
    "\n",
    "#For distance between 12 and 14th point\n",
    "\n",
    "x_max = 640, y_max =480\n",
    "\n",
    "#The x_max and the y_max depends on your camera pixels intensity.\n",
    "\n",
    "p1 = [keypoints[12]['X']*640, keypoints[12]['Y']*480]\n",
    "q1 = [keypoints[14]['X']*640, keypoints[14]['Y']*480]\n",
    "\n",
    "left_upper_arm_length = math.dist(p1,q1)\n",
    "\n",
    "\n",
    "p2 = [keypoints[16]['X']*640, keypoints[16]['Y']*480]\n",
    "q2 = [keypoints[14]['X']*640, keypoints[14]['Y']*480]\n",
    "\n",
    "left_forearm_length = math.dist(p2,q2)\n",
    "\n",
    "\n",
    "print(left_upper_arm_length, left_forearm_length)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f051b40",
   "metadata": {},
   "source": [
    "Right arm pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12632944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.746493588777746 53.8655628514685\n"
     ]
    }
   ],
   "source": [
    "#For right arm \n",
    "\n",
    "#For distance betwen 11, 13th point\n",
    "\n",
    "\n",
    "p1 = [keypoints[11]['X']*640, keypoints[11]['Y']*480]\n",
    "q1 = [keypoints[13]['X']*640, keypoints[13]['Y']*480]\n",
    "\n",
    "right_upper_arm_length = math.dist(p1,q1)\n",
    "\n",
    "#distance between 13 and 15th point\n",
    "\n",
    "p2 = [keypoints[13]['X']*640, keypoints[13]['Y']*480]\n",
    "q2 = [keypoints[15]['X']*640, keypoints[15]['Y']*480]\n",
    "\n",
    "right_forearm_length = math.dist(p2,q2)\n",
    "\n",
    "\n",
    "print(right_upper_arm_length, right_forearm_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f9475",
   "metadata": {},
   "source": [
    "Left to right shoulder Width pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b566c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left to right Shoulder Width is 83.50189587815508\n"
     ]
    }
   ],
   "source": [
    "p1 = [keypoints[11]['X']*640, keypoints[11]['Y']*480]\n",
    "q1 = [keypoints[12]['X']*640, keypoints[12]['Y']*480]\n",
    "\n",
    "lr_shoulder_width = math.dist(p1,q1)\n",
    "\n",
    "print(\"Left to right Shoulder Width is \" + str(lr_shoulder_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33c611",
   "metadata": {},
   "source": [
    "Waist width pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9798799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waist Width is 48.959270675390165\n"
     ]
    }
   ],
   "source": [
    "p1 = [keypoints[24]['X']*640, keypoints[24]['Y']*480]\n",
    "q1 = [keypoints[23]['X']*640, keypoints[23]['Y']*480]\n",
    "\n",
    "waist_width = math.dist(p1,q1)\n",
    "\n",
    "print(\"Waist Width is \" + str(waist_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f85dbe",
   "metadata": {},
   "source": [
    "Left upper leg length pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "045496a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left upper leg length is 105.15454897021448\n"
     ]
    }
   ],
   "source": [
    "p1 = [keypoints[24]['X']*640, keypoints[24]['Y']*480]\n",
    "q1 = [keypoints[26]['X']*640, keypoints[26]['Y']*480]\n",
    "\n",
    "left_upper_leg= math.dist(p1,q1)\n",
    "\n",
    "print(\"Left upper leg length is \" + str(left_upper_leg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d2554a",
   "metadata": {},
   "source": [
    "Left lower leg length pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8be751b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left lower leg length is 96.15182385693468\n"
     ]
    }
   ],
   "source": [
    "p1 = [keypoints[28]['X']*640, keypoints[28]['Y']*480]\n",
    "q1 = [keypoints[26]['X']*640, keypoints[26]['Y']*480]\n",
    "\n",
    "left_lower_leg= math.dist(p1,q1)\n",
    "\n",
    "print(\"left lower leg length is \" + str(left_lower_leg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9d594",
   "metadata": {},
   "source": [
    "Right upper leg length pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a7ef3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right upper leg length is 99.94115211007603\n"
     ]
    }
   ],
   "source": [
    "p1 = [keypoints[23]['X']*640, keypoints[23]['Y']*480]\n",
    "q1 = [keypoints[25]['X']*640, keypoints[25]['Y']*480]\n",
    "\n",
    "right_upper_leg= math.dist(p1,q1)\n",
    "\n",
    "print(\"Right upper leg length is \" + str(right_upper_leg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929fe9e",
   "metadata": {},
   "source": [
    "Right lower leg length pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fab2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right upper leg length is 88.98645491248645\n"
     ]
    }
   ],
   "source": [
    "p1 = [keypoints[27]['X']*640, keypoints[27]['Y']*480]\n",
    "q1 = [keypoints[25]['X']*640, keypoints[25]['Y']*480]\n",
    "\n",
    "right_lower_leg= math.dist(p1,q1)\n",
    "\n",
    "print(\"Right upper leg length is \" + str(right_lower_leg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an image\n",
    "img = cv2.imread(\"image.jpg\")\n",
    "\n",
    "# Specify the known distance between two points in the image (in real-world units)\n",
    "known_distance = 24.0 # inches\n",
    "\n",
    "# Specify the coordinates of the two points in the image\n",
    "pt1 = (200, 200)\n",
    "pt2 = (300, 200)\n",
    "\n",
    "# Calculate the Euclidean distance between the two points in pixels\n",
    "pixel_distance = cv2.norm(pt1, pt2, cv2.NORM_L2)\n",
    "\n",
    "# Calculate the length of one pixel in real-world units\n",
    "pixel_length = known_distance / pixel_distance\n",
    "\n",
    "print(\"The length of one pixel is\", pixel_length, \"inches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82981919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    # Read an image, flip it around y-axis for correct handedness output (see\n",
    "    # above).\n",
    "    image = cv2.flip(cv2.imread(file), 1)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print handedness and draw hand landmarks on the image.\n",
    "    print('Handedness:', results.multi_handedness)\n",
    "    if not results.multi_hand_landmarks:\n",
    "      continue\n",
    "    image_height, image_width, _ = image.shape\n",
    "    annotated_image = image.copy()\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "      print('hand_landmarks:', hand_landmarks)\n",
    "      print(\n",
    "          f'Index finger tip coordinates: (',\n",
    "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "      )\n",
    "      mp_drawing.draw_landmarks(\n",
    "          annotated_image,\n",
    "          hand_landmarks,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())\n",
    "    cv2.imwrite(\n",
    "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
    "    # Draw hand world landmarks.\n",
    "    if not results.multi_hand_world_landmarks:\n",
    "      continue\n",
    "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
    "      mp_drawing.plot_landmarks(\n",
    "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6854b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "keypoints = []\n",
    "for data_point in hand_landmarks.landmark:\n",
    "    keypoints.append({'X':data_point.x, 'Y': data_point.y, 'Z':data_point.z, 'visibility': data_point.visibility,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e163f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keypoints_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e965cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.64395611187196 88.17626177704558 96.86685097677939 89.75537619469151 71.86007930865979\n"
     ]
    }
   ],
   "source": [
    "#For left arm (12, 14, 16)\n",
    "\n",
    "#For distance between 12 and 14th point\n",
    "\n",
    "p1 = [keypoints[8]['X']*640, keypoints[8]['Y']*480]\n",
    "q1 = [keypoints[5]['X']*640, keypoints[5]['Y']*480]\n",
    "\n",
    "index_finger = math.dist(p1,q1)\n",
    "\n",
    "\n",
    "p2 = [keypoints[4]['X']*640, keypoints[4]['Y']*480]\n",
    "q2 = [keypoints[2]['X']*640, keypoints[2]['Y']*480]\n",
    "\n",
    "thumb_finger = math.dist(p2,q2)\n",
    "\n",
    "\n",
    "p2 = [keypoints[9]['X']*640, keypoints[9]['Y']*480]\n",
    "q2 = [keypoints[12]['X']*640, keypoints[12]['Y']*480]\n",
    "\n",
    "middle_finger = math.dist(p2,q2)\n",
    "\n",
    "\n",
    "p2 = [keypoints[16]['X']*640, keypoints[16]['Y']*480]\n",
    "q2 = [keypoints[13]['X']*640, keypoints[13]['Y']*480]\n",
    "\n",
    "ring_finger = math.dist(p2,q2)\n",
    "\n",
    "\n",
    "p2 = [keypoints[17]['X']*640, keypoints[17]['Y']*480]\n",
    "q2 = [keypoints[20]['X']*640, keypoints[20]['Y']*480]\n",
    "\n",
    "pinky_finger = math.dist(p2,q2)\n",
    "\n",
    "\n",
    "print(thumb_finger, index_finger, middle_finger, ring_finger, pinky_finger)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
